{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.python.platform import app\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "\n",
    "\n",
    "# How often to record tensorboard summaries.\n",
    "SUMMARY_INTERVAL = 40\n",
    "\n",
    "# How often to run a batch through the validation model.\n",
    "VAL_INTERVAL = 200\n",
    "\n",
    "# How often to save a model checkpoint\n",
    "SAVE_INTERVAL = 400\n",
    "\n",
    "# tf record data location:\n",
    "DATA_DIR = '/home/xca64/vml4/dataset/ucf101/ucf101_imgs'\n",
    "\n",
    "# local output directory\n",
    "OUT_DIR = '/cs/vml4/xca64/robot_data/result'\n",
    "\n",
    "# summary output dir\n",
    "SUM_DIR = '/cs/vml4/xca64/robot_data/summaries'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('data_dir', DATA_DIR, 'directory containing data.')\n",
    "flags.DEFINE_string('dataset_name', 'ucf', 'dataset used')\n",
    "flags.DEFINE_string('output_dir', OUT_DIR, 'directory for model checkpoints.')\n",
    "flags.DEFINE_string('gif_dir', '/cs/vml4/xca64/robot_data/gif/' , 'directory gif result')\n",
    "flags.DEFINE_integer('gif_nums', 5 , 'number of gif files to save')\n",
    "flags.DEFINE_string('event_log_dir',SUM_DIR, 'directory for writing summary.')\n",
    "flags.DEFINE_integer('num_iterations', 100000, 'number of training iterations.')\n",
    "flags.DEFINE_string('pretrained_model', '' ,\n",
    "                    'filepath of a pretrained model to initialize from.')\n",
    "\n",
    "flags.DEFINE_integer('sequence_length', 10,\n",
    "                     'sequence length, including context frames.')\n",
    "flags.DEFINE_integer('context_frames', 2, '# of frames before predictions.')\n",
    "flags.DEFINE_integer('use_state', 1,\n",
    "                     'Whether or not to give the state+action to the model')\n",
    "\n",
    "flags.DEFINE_string('model', 'prednet',\n",
    "                    'model architecture to use - prediction, prednet')\n",
    "\n",
    "flags.DEFINE_string('optimizer', 'ADAM',\n",
    "                    'model architecture to use - prediction, prednet')\n",
    "\n",
    "flags.DEFINE_integer('num_masks', 10,\n",
    "                     'number of masks, usually 1 for DNA, 10 for CDNA, STN.')\n",
    "flags.DEFINE_float('schedsamp_k', 900.0,\n",
    "                   'The k hyperparameter for scheduled sampling,'\n",
    "                   '-1 for no scheduled sampling.')\n",
    "flags.DEFINE_float('train_val_split', 0.95,\n",
    "                   'The percentage of files to use for the training set,'\n",
    "                   ' vs. the validation set.')\n",
    "\n",
    "flags.DEFINE_float('gpu_memory_fraction', 1.0,\n",
    "                   'gpu percentage')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 32, 'batch size for training')\n",
    "flags.DEFINE_float('learning_rate', 0.001,\n",
    "                   'the base learning rate of the generator')\n",
    "\n",
    "#####################\n",
    "# Fine-Tuning Flags #\n",
    "#####################\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint_path', '/home/xca64/vml4/resnet/model/resnet_v1_50.ckpt',\n",
    "    'The path to a checkpoint from which to fine-tune.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint_exclude_scopes', 'resnet_v1_50/logits,predictions',\n",
    "    'Comma-separated list of scopes of variables to exclude when restoring '\n",
    "    'from a checkpoint.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'trainable_scopes', None,\n",
    "    'Comma-separated list of scopes to filter the set of variables to train.'\n",
    "    'By default, None would train all the variables.')\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    'ignore_missing_vars', True,\n",
    "    'When restoring a checkpoint would ignore missing variables.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/xca64/vml4/github/video_prediction')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.ucf101_img_input import build_tfrecord_input\n",
    "from data.ucf101_img_input import get_image_paths_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images, labels = build_tfrecord_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to run resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import control_flow_ops\n",
    "slim = tf.contrib.slim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = slim.one_hot_encoding(\n",
    "          tf.string_to_number(labels, out_type=tf.int32), 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.slim.nets import resnet_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs has shape [batch, 224, 224, 3]\n",
    "with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "  net, end_points = resnet_v1.resnet_v1_50(images, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Fine-tuning from /home/xca64/vml4/resnet/model/resnet_v1_50.ckpt\n"
     ]
    }
   ],
   "source": [
    "init_from_checkpoint = _get_init_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "    labels,tf.squeeze(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_op = train(cross_entropy, global_step, FLAGS.optimizer, FLAGS.learning_rate, 0.9999, _get_variables_to_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-4, started daemon 140574598354688)>,\n",
       " <Thread(Thread-5, started daemon 140574589961984)>,\n",
       " <Thread(Thread-6, started daemon 140574581569280)>,\n",
       " <Thread(Thread-7, started daemon 140574573176576)>,\n",
       " <Thread(Thread-8, started daemon 140574564783872)>,\n",
       " <Thread(Thread-9, started daemon 140573809833728)>,\n",
       " <Thread(Thread-10, started daemon 140573801441024)>,\n",
       " <Thread(Thread-11, started daemon 140573793048320)>,\n",
       " <Thread(Thread-12, started daemon 140573784655616)>,\n",
       " <Thread(Thread-13, started daemon 140573776262912)>,\n",
       " <Thread(Thread-14, started daemon 140573767870208)>,\n",
       " <Thread(Thread-15, started daemon 140573759477504)>,\n",
       " <Thread(Thread-16, started daemon 140573205853952)>,\n",
       " <Thread(Thread-17, started daemon 140573197461248)>,\n",
       " <Thread(Thread-18, started daemon 140573189068544)>,\n",
       " <Thread(Thread-19, started daemon 140573180675840)>,\n",
       " <Thread(Thread-20, started daemon 140573172283136)>,\n",
       " <Thread(Thread-21, started daemon 140573163890432)>,\n",
       " <Thread(Thread-22, started daemon 140573155497728)>,\n",
       " <Thread(Thread-23, started daemon 140572668983040)>,\n",
       " <Thread(Thread-24, started daemon 140572660590336)>,\n",
       " <Thread(Thread-25, started daemon 140572652197632)>,\n",
       " <Thread(Thread-26, started daemon 140572643804928)>,\n",
       " <Thread(Thread-27, started daemon 140572635412224)>,\n",
       " <Thread(Thread-28, started daemon 140572627019520)>,\n",
       " <Thread(Thread-29, started daemon 140572618626816)>,\n",
       " <Thread(Thread-30, started daemon 140572132112128)>,\n",
       " <Thread(Thread-31, started daemon 140572123719424)>,\n",
       " <Thread(Thread-32, started daemon 140572115326720)>,\n",
       " <Thread(Thread-33, started daemon 140572106934016)>,\n",
       " <Thread(Thread-34, started daemon 140572098541312)>,\n",
       " <Thread(Thread-35, started daemon 140572090148608)>,\n",
       " <Thread(Thread-36, started daemon 140572081755904)>,\n",
       " <Thread(Thread-37, started daemon 140571595241216)>,\n",
       " <Thread(Thread-38, started daemon 140571586848512)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "coord = tf.train.Coordinator()\n",
    "tf.train.start_queue_runners(coord=coord, sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_from_checkpoint(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0654087"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err, _ = sess.run([cross_entropy, train_op])\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"Add summaries for losses.\n",
    "  \n",
    "    Generates moving average for all losses and associated summaries for\n",
    "    visualizing the performance of the network.\n",
    "  \n",
    "    Args:\n",
    "      total_loss: Total loss from loss().\n",
    "    Returns:\n",
    "      loss_averages_op: op for generating moving averages of losses.\n",
    "    \"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "#     losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply([total_loss])\n",
    "  \n",
    "    # Attach a scalar summmary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name +'raw', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "  \n",
    "    return loss_averages_op\n",
    "\n",
    "def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        if optimizer=='ADAGRAD':\n",
    "            opt = tf.train.AdagradOptimizer(learning_rate)\n",
    "        elif optimizer=='ADADELTA':\n",
    "            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n",
    "        elif optimizer=='ADAM':\n",
    "            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n",
    "        elif optimizer=='RMSPROP':\n",
    "            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n",
    "        elif optimizer=='MOM':\n",
    "            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimization algorithm')\n",
    "    \n",
    "        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n",
    "        \n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "  \n",
    "    # Add histograms for trainable variables.\n",
    "    if log_histograms:\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "   \n",
    "    # Add histograms for gradients.\n",
    "    if log_histograms:\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "  \n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        moving_average_decay, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "  \n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "  \n",
    "    return train_op\n",
    "\n",
    "def _get_variables_to_train():\n",
    "  \"\"\"Returns a list of variables to train.\n",
    "\n",
    "  Returns:\n",
    "    A list of variables to train by the optimizer.\n",
    "  \"\"\"\n",
    "  if FLAGS.trainable_scopes is None:\n",
    "    return tf.trainable_variables()\n",
    "  else:\n",
    "    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(',')]\n",
    "\n",
    "  variables_to_train = []\n",
    "  for scope in scopes:\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "    variables_to_train.extend(variables)\n",
    "  return variables_to_train\n",
    "\n",
    "def _get_init_fn():\n",
    "  \"\"\"Returns a function run by the chief worker to warm-start the training.\n",
    "\n",
    "  Note that the init_fn is only run when initializing the model during the very\n",
    "  first global step.\n",
    "\n",
    "  Returns:\n",
    "    An init function run by the supervisor.\n",
    "  \"\"\"\n",
    "  if FLAGS.checkpoint_path is None:\n",
    "    return None\n",
    "\n",
    "  # Warn the user if a checkpoint exists in the train_dir. Then we'll be\n",
    "#   # ignoring the checkpoint anyway.\n",
    "#   if tf.train.latest_checkpoint(FLAGS.train_dir):\n",
    "#     tf.logging.info(\n",
    "#         'Ignoring --checkpoint_path because a checkpoint already exists in %s'\n",
    "#         % FLAGS.train_dir)\n",
    "#     return None\n",
    "\n",
    "  exclusions = []\n",
    "  if FLAGS.checkpoint_exclude_scopes:\n",
    "    exclusions = [scope.strip()\n",
    "                  for scope in FLAGS.checkpoint_exclude_scopes.split(',')]\n",
    "\n",
    "  # TODO(sguada) variables.filter_variables()\n",
    "  variables_to_restore = []\n",
    "  for var in slim.get_model_variables():\n",
    "    excluded = False\n",
    "    for exclusion in exclusions:\n",
    "      if var.op.name.startswith(exclusion):\n",
    "        excluded = True\n",
    "        break\n",
    "    if not excluded:\n",
    "      variables_to_restore.append(var)\n",
    "\n",
    "  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n",
    "    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n",
    "  else:\n",
    "    checkpoint_path = FLAGS.checkpoint_path\n",
    "\n",
    "  tf.logging.info('Fine-tuning from %s' % checkpoint_path)\n",
    "\n",
    "  return slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      variables_to_restore,\n",
    "      ignore_missing_vars=FLAGS.ignore_missing_vars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
